# DreamerV2 PyTorch Implementation

This project is a PyTorch implementation of the DreamerV2 agent, a reinforcement learning algorithm that learns a world model from pixels and uses it to train an actor-critic agent in imagined trajectories. This implementation is designed for solving Atari 2600 environments from the Gymnasium library.

## Project Structure

The repository is organized as follows:

```
.
├── configs/            # YAML configuration files for different environments.
│   ├── bankheist.yaml
│   ├── freeway.yaml
│   └── pong.yaml
├── src/                # Main source code directory.
│   ├── envs/           # Environment wrappers.
│   │   └── atari.py    # Wrapper for Atari environments.
│   ├── models/         # Core neural network models.
│   │   ├── rssm.py     # Recurrent State-Space Model (the world model).
│   │   ├── vision.py   # Encoder and Decoder for image processing.
│   │   ├── actor_critic.py # Actor and Critic networks.
│   │   ├── behavior.py # Logic for training in imagined trajectories.
│   │   └── heads.py    # Reward and Discount predictors.
│   ├── replay/         # Replay buffer implementation.
│   │   └── replay_buffer.py
│   ├── utils/          # Utility functions.
│   │   ├── image_saver.py
│   │   └── tools.py
│   └── train.py        # Main script for training the agent.
...
```

-   **`configs/`**: Contains all hyperparameter configurations. You can create new `.yaml` files to experiment with different environments or model settings.
-   **`src//`**: The core logic of the agent.
    -   **`train.py`**: The main entry point to start training an agent.
    -   **`models/`**: This directory contains the key components of the DreamerV2 architecture. The `rssm.py` is the heart of the world model, `vision.py` handles the visual processing, and `actor_critic.py` defines the agent's policy and value functions.
    -   **`envs/`**: Contains wrappers to interface with the learning environments (e.g., Atari).
    -   **`replay/`**: Stores and samples past experiences for training.

## How It Works

1.  **World Model**: The agent learns a "world model" (`RSSM`) directly from the pixels of the environment. This model learns to predict the consequences of actions in a compact latent space. The world model consists of:
    -   An `Encoder` to compress the image observations.
    -   A recurrent model (`GRUCell`) to predict future latent states.
    -   A `Decoder` to reconstruct images from latent states (for training the encoder).
    -   Predictor heads (`RewardPredictor`, `DiscountPredictor`) to predict future rewards and episode termination.

2.  **Imagined Trajectories**: Once the world model is trained, the agent uses it to efficiently train the `Actor` and `Critic` networks by imagining future trajectories purely in latent space. This is much faster than collecting real experience from the environment.

3.  **Actor-Critic**: The `Actor` learns a policy to maximize future rewards, while the `Critic` learns to estimate the value of being in a particular state. Both are trained on the imagined trajectories generated by the world model.

## Usage

To start training, run the `train.py` script with the desired configuration file.

```bash
python -m src.train --config configs/pong.yaml
```

You can monitor training progress and results if you have `wandb` configured.

## Dependencies

This project requires the following Python libraries. You can install them using pip:

```bash
pip install -r requirements.txt
```
